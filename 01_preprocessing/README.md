# Preprocessing

## Software needed to download the data

Please use the Rivas Lab module. The current version is `ukbb-showcase-utils/201903`.

We had our copy of the programs in this repo, but they are moved to other places in `$OAK`.

```
[ytanigaw@sh-102-07 ~/repos/rivas-lab/ukbb-tools/01_preprocessing]$ md5sum ukb*
06b5b39ab608a093b1dd264e27280316  ukbconv
254669d4bf6fdf8ba66d5bc2c9c5633c  ukb_unpack
[ytanigaw@sh-102-07 /oak/stanford/groups/mrivas/software/ukbb-showcase-utils]$ md5sum ./201804/ukb_unpack
254669d4bf6fdf8ba66d5bc2c9c5633c  ./201804/ukb_unpack
[ytanigaw@sh-102-07 /oak/stanford/groups/mrivas/software/ukbb-showcase-utils]$ find . -name ukbconv -exec md5sum {} \;
06b5b39ab608a093b1dd264e27280316  ./201903/ukbconv
06b5b39ab608a093b1dd264e27280316  ./201810/ukbconv
```

## Downloading the data

The PI (in this case, Manny) has the sole privilege of being able to request and download new datasets. For information about how to apply for new data, please go [here]().

Once the data is downloaded, clone this directory to wherever you are working.

## Unpacking/decrypting the data

[The official documentation (Using UKB data)](http://biobank.ctsu.ox.ac.uk/showcase/docs/UsingUKBData.pdf) explains this procedure.

`ukb_unpack` converts the raw files from biobank (which are `.enc` files) into `.enc_ukb` files, which are the decrypted version. In order to unpack the data, you will need to do three things:

1) `cd` to the directory in which the `.enc` file is.
2) Note the absolute path to the `ukb_unpack` program in the cloned `preprocessing` directory, or the relative path from the directory containing the `.enc` file to the `ukb_unpack` program.
3) There should be an Application ID (most likely 24983 for us) and a Run ID associated with each application for new data. When the application is approved, Manny should get an email containing a password for the dataset. Note this password down.

Finally, run the following command:

`/path/to/ukb_unpack ukbXXXX.enc password`

Depending on the size and complexity of the dataset, you could be looking at a couple of minutes' runtime. A `.enc_ukb` file should pop up in the same directory you are currently in - `ls` to check.

## Converting the data

`ukbconv` converts the `.enc_ukb` files just generated by the `ukb_unpack` program into `.tab` and other formats. Run the following command from the same directory as before, replacing `ukbXXXX.enc_ukb` with the name of your `.enc_ukb` file:

`/path/to/ukbconv ukbXXXX.enc_ukb R`

When the command finishes running, you will see that a couple of files (most importantly, a `.tab` file) is generated in the directory. This file will be used in downstream analyses. *NOTE*: Depending on the size and complexity of the dataset and the machine you are running on (I personally recommend 32GB RAM, 4 CPUs (`sdev -p mrivas -t 7-0:00:00 -m 32g -n4`)), you could be looking at a couple of hours' runtime. Run these conversion commands in `screen` or `tmux` if possible so that they can run in the background.

## (Optional) Generating a `.tab.columns` file

In order to save a list of fields that the dataset contains, you will want to generate a `.tab.columns` file from the `.tab` file you just created. This will provide you an easy way to look at all the fields in the file, but is not strictly necessary for the pipeline. Run the following to generate it within your directory, replacing `ukbXXXX.tab` with the name of your `.tab` file:

```
ml load ukbb-showcase-utils
ukbtabcols.sh ukbXXXXX.tab > ukbXXXXX.tab.columns
```

## (Optional) Generating a `.tab.columns.summary.tsv.gz` file

To provide column-wise summary of the data table file, you may run `compute_col-wise_md5sum.sh`. 

```
bash compute_col-wise_md5sum.sh ukbXXXXX.tab > ukbXXXXX.tab.columns.summary.tsv
gzip -9 ukbXXXXX.tab.columns.summary.tsv
```

In case the table file is large, there are options to compute the statistics for selected columns (`-c` option) or for a selected range (`-s` and `-e` options to indicate the start and the end of the range (both inclusive)). Please also look at the job directory to see the example.

The most recent version of the results are saved to `/oak/stanford/groups/mrivas/ukbb24983/phenotypedata/master_phe/tab.columns.summary.tsv.gz`.


## (Optional) Mapping IDs across different application IDs

```
# map IDs
$ cat ukb27645.tab | awk '(NR>1)' | awk '{print NR, $1}'| sort -k2b,2 | join -a 1 -1 2 -2 2 /dev/stdin <( cat /oak/stanford/groups/mrivas/private_data/ukbb/24983/sqc/ukb24983_16698_mapping.tsv | sort -k2b,2 ) | sort -k2n,2 | awk -v OFS='\t' '{print $3, $1}' > ukb27645.tab.ukb24983_16698.eids.tsv
# you should check the order of the IDs (on 16698) is the same
$ cat ukb27645.tab | cut -f1 | awk 'NR>1' | md5sum
2c50d482c3bf4aeee7a01ae19c447ffd  -
$ cat ukb27645.tab.ukb24983_16698.eids.tsv | awk -v FS='\t' '{print $2}' | md5sum
2c50d482c3bf4aeee7a01ae19c447ffd  -
# you should check if the mapping is complete (has no missing value)
$ cat ukb27645.tab.ukb24983_16698.eids.tsv | awk -v FS='\t' '{print NF}' | uniq -c
 502536 2
# map the table
$ cat ukb27645.tab | awk 'NR == 1' > ukb27645.mapped_to_ukb24983.tab
$ paste <(cat ukb27645.tab.ukb24983_16698.eids.tsv  | awk '{print $1}') <(cat ukb27645.tab | awk 'NR>1' | cut -f2- ) >> ukb27645.mapped_to_ukb24983.tab
# final check
$ cat ukb27645.mapped_to_ukb24983.tab | cut -f2- | md5sum
204318e13b9c32a2d9b7c49e7965b81f  -
$ cat ukb27645.tab | cut -f2- | md5sum
204318e13b9c32a2d9b7c49e7965b81f  -
$ cat ukb27645.mapped_to_ukb24983.tab | awk -v FS='\t' '{print NF}' | uniq -c
 502537 2265
$ cat ukb27645.tab | awk -v FS='\t' '{print NF}' | uniq -c
 502537 2265
```

## Defining phenotypes

With the `.tab.columns` file in tow, you're ready to start [defining phenotypes](https://github.com/rivas-lab/ukbb-tools/tree/master/02_phenotyping).

