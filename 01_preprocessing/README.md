# Preprocessing

The UK Biobank is organized in a semi-hierarchical structure, with data placed into top-level "baskets" and further down into "tables." You can find [here](http://bit.ly/UKB24983_tables) a list of tables and their respective baskets, descriptions, release dates, and paths on Sherlock.

The contents of this folder are meant to turn encoded files from the UK Biobank (`.enc`) into `.tab` files that are readable (and thus usable) by the lab.

## Contents

1. [`01_preprocessing_auto.sh`](https://github.com/rivas-lab/ukbb-tools/blob/master/01_preprocessing/01_preprocessing_auto.sh): Automatic UK Biobank decryption script.
- Inputs: `.enc`, `.key` (both from UK Biobank)
- Outputs: `.tab`, `.tab.columns`, `.tab.log`, `.tab.columns.summary.tsv.gz` 
- Example usage: `01_preprocessing_auto.sh <ukbXXXX.enc> <k24983.key>`
- See ["Pipelines and Workflows: Unpacking/decrypting/converting the data"](https://github.com/rivas-lab/ukbb-tools/blob/master/01_preprocessing#unpackingdecryptingconverting-the-data) for more details
2. [`compute_col-wise_md5sum.sh`](https://github.com/rivas-lab/ukbb-tools/blob/master/01_preprocessing/compute_col-wise_md5sum.sh): Computes md5 checksums for the columns in a table.
- Inputs: `.tab` (as created using [`01_preprocessing_auto.sh`](https://github.com/rivas-lab/ukbb-tools/blob/master/01_preprocessing/01_preprocessing_auto.sh))
- Outputs: `.tab.columns.summary.tsv.gz`
- Example usage: `bash compute_col-wise_md5sum.sh ukbXXXXX.tab > ukbXXXXX.tab.columns.summary.tsv`
- See ["Pipelines and Workflows: Generating a `.tab.columns.summary.tsv.gz` file"](https://github.com/rivas-lab/ukbb-tools/blob/master/01_preprocessing#generating-a-tabcolumnssummarytsvgz-file) for more details
3. [compute_col-wise_md5sum_ary_job.sh](https://github.com/rivas-lab/ukbb-tools/blob/master/01_preprocessing/compute_col-wise_md5sum_ary_job_task.sh): Job wrapper for [`compute_col-wise_md5sum.sh`](https://github.com/rivas-lab/ukbb-tools/blob/master/01_preprocessing/compute_col-wise_md5sum.sh).
- Inputs: `in_file`, `task_id`, `batch_size` 
4. [compute_col-wise_md5sum_job](https://github.com/rivas-lab/ukbb-tools/tree/master/01_preprocessing/compute_col-wise_md5sum_job): SLURM job wrapper scripts for [`compute_col-wise_md5sum.sh`](https://github.com/rivas-lab/ukbb-tools/blob/master/01_preprocessing/compute_col-wise_md5sum.sh).

## Pipelines and Workflows

### Unpacking/decrypting/converting the data

The UK Biobank offers two programs, `ukbconv` and `ukbunpack`, that have now been packaged into a lab module on Sherlock called `ukbb-showcase-utils`. `ukbunpack` converts the raw, encrypted files from the UK Biobank (which are `.enc` files) into `.enc_ukb` files, which are the decrypted version. `ukbconv` converts the `.enc_ukb` files generated by the `ukb_unpack` program into `.tab` (and other) formats.

In order to unpack the data coming in for a given table, you will need to do three things:

1) `cd` to the directory in which the `.enc` file is. In our directory structure, they should be located in `/oak/stanford/groups/mrivas/ukbb24983/phenotypedata/[basket]/[table]/download`.
2) There should be an Application ID (most likely 24983 for us) and a Run ID associated with each application for new data. When the application is approved, Manny should get an email containing a password for the dataset. Download this password file to the same directory as the `.enc` file; it should have a `.key` suffix.
3) Finally, run the following command:

```{bash}
01_preprocessing_auto.sh <ukbXXXX.enc> <k24983.key>
```

This is a wrapper script that loads the `ukbb-showcase-utils` module and (most importantly) runs `ukbunpack` and `ukbconv`, among other scripts.

When the command finishes running, you will see that a couple of files have been generated in the directory:
- `ukbXXXX.tab`: The tab file that is indexed by individual ID (IID) for rows and field/instance/array index. For example, `f.92.0.0` represents the first instance of field 92, and `f.92.1.0` represents the second; `f.92.0.1` represents the second "data sub-field" within field 92 (which in this case is "Operation year/age first occurred", so the second operation for which this field was smeasured). This file will be used in downstream analyses.
- `ukbXXXX.tab.columns`: File that contains all fields, instances, and array indices (as described above) in the table as separate columns.
- `ukbXXXX.tab.columns.summary.tsv.gz`: File that is generated as a result of a call to [`compute_col-wise_md5sum.sh`](https://github.com/rivas-lab/ukbb-tools/blob/master/01_preprocessing/compute_col-wise_md5sum.sh). This file contains indices, names, an `md5sum`, number of non-NA values, number of unique values, inferred data type, and 

*NOTE*: Depending on the size and complexity of the dataset and the machine you are running on (weÃŸ personally recommend 32GB RAM, 4 CPUs (`sdev -p mrivas -t 7-0:00:00 -m 32g -n4`)), you could be looking at a couple of hours' runtime. Run the script in `screen` or `tmux` if possible, or submit to `sbatch`, so that it can run in the background.

## Generating a `.tab.columns.summary.tsv.gz` file

To provide a column-wise summary of any data table file (including master phenotype files), you can run `compute_col-wise_md5sum.sh` using the following set of commands:

```{bash}
bash compute_col-wise_md5sum.sh ukbXXXXX.tab > ukbXXXXX.tab.columns.summary.tsv
gzip -9 ukbXXXXX.tab.columns.summary.tsv
```

In case the table file is large, there are options to compute the statistics for selected columns (`-c` option) or for a selected range of column indices (`-s` and `-e` options to indicate the start and the end of the range (both inclusive)). Please look at the [job directory]([compute_col-wise_md5sum_job](https://github.com/rivas-lab/ukbb-tools/tree/master/01_preprocessing/compute_col-wise_md5sum_job)) to see an example.

This script historically has been run on the `master.phe` file. The most recent version of the results of this script are saved to a symlink, `/oak/stanford/groups/mrivas/ukbb24983/phenotypedata/master_phe/tab.columns.summary.tsv.gz`.

## (Optional) Mapping IDs across different application IDs

```
# map IDs
$ cat ukb27645.tab | awk '(NR>1)' | awk '{print NR, $1}'| sort -k2b,2 | join -a 1 -1 2 -2 2 /dev/stdin <( cat /oak/stanford/groups/mrivas/private_data/ukbb/24983/sqc/ukb24983_16698_mapping.tsv | sort -k2b,2 ) | sort -k2n,2 | awk -v OFS='\t' '{print $3, $1}' > ukb27645.tab.ukb24983_16698.eids.tsv
# you should check the order of the IDs (on 16698) is the same
$ cat ukb27645.tab | cut -f1 | awk 'NR>1' | md5sum
2c50d482c3bf4aeee7a01ae19c447ffd  -
$ cat ukb27645.tab.ukb24983_16698.eids.tsv | awk -v FS='\t' '{print $2}' | md5sum
2c50d482c3bf4aeee7a01ae19c447ffd  -
# you should check if the mapping is complete (has no missing value)
$ cat ukb27645.tab.ukb24983_16698.eids.tsv | awk -v FS='\t' '{print NF}' | uniq -c
 502536 2
# map the table
$ cat ukb27645.tab | awk 'NR == 1' > ukb27645.mapped_to_ukb24983.tab
$ paste <(cat ukb27645.tab.ukb24983_16698.eids.tsv  | awk '{print $1}') <(cat ukb27645.tab | awk 'NR>1' | cut -f2- ) >> ukb27645.mapped_to_ukb24983.tab
# final check
$ cat ukb27645.mapped_to_ukb24983.tab | cut -f2- | md5sum
204318e13b9c32a2d9b7c49e7965b81f  -
$ cat ukb27645.tab | cut -f2- | md5sum
204318e13b9c32a2d9b7c49e7965b81f  -
$ cat ukb27645.mapped_to_ukb24983.tab | awk -v FS='\t' '{print NF}' | uniq -c
 502537 2265
$ cat ukb27645.tab | awk -v FS='\t' '{print NF}' | uniq -c
 502537 2265
```

## Defining phenotypes

With the `.tab.columns` file in tow, you're ready to start [defining phenotypes](https://github.com/rivas-lab/ukbb-tools/tree/master/02_phenotyping).