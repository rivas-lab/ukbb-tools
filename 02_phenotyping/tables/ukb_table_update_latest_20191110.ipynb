{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "import functools\n",
    "\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"Annotator\", \"Annotation date\", \"Name\", \"GBE ID\", \"Field\", \"BasketID\", \"TableID\", \n",
    "    \"FieldID\", \"QT_total_num\", \"BIN_total_num\", \"QT_index\", \"BIN_index\", \"coding_exclude\", \n",
    "    \"coding_QT\", \"coding_binary_case\", \"coding_binary_control\", \"Participants\", \"Stability\", \n",
    "    \"ValueType\", \"Units\", \"Strata\", \"Sexed\", \"Instances\", \"Array\", \"Coding\", \"Link\", \"Notes\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_read_table(tbl, cols):\n",
    "    df = pd.read_table(tbl, index_col=None, dtype=object)\n",
    "    for c in set(cols) - set(df.columns):\n",
    "        df[c]=None\n",
    "    df['Source_file'] = tbl\n",
    "    return(df[cols + ['Source_file']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_table(existing, new, key_col, field_col='FieldID'):\n",
    "    # take Field IDs that are already annotated (by a user-specified column)\n",
    "    annotated = set(existing[\n",
    "        (existing[key_col] is not None) and \n",
    "        (existing[key_col] != '')\n",
    "    ][field_col].map(lambda x: str(x)))\n",
    "    return(pd.concat([\n",
    "        existing, \n",
    "        new[new[field_col].map(lambda x: str(x) not in annotated)]\n",
    "    ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_tables = [\n",
    "    'ukb_20170727.tsv', 'ukb_20170818.tsv', 'ukb_20170827.tsv', 'ukb_20171015.tsv', \n",
    "    'ukb_20171110.tsv', 'ukb_20171113.tsv', 'ukb_20171211.tsv', 'ukb_20181109.tsv', \n",
    "    'ukb_20190327.tsv', 'ukb_20190406.tsv', 'ukb_20190409.tsv', 'ukb_20190412.tsv'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual inspection of duplicated entries in the exsiting tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_df_simple_concat = pd.concat(\n",
    "    [my_read_table(x, cols) for x in existing_tables]\n",
    ")\n",
    "dup_entries = {k:v for k,v in collections.Counter(existing_df_simple_concat['GBE ID']).items() if v>1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_df_simple_concat[\n",
    "    existing_df_simple_concat['GBE ID'].isin(dup_entries.keys())\n",
    "].sort_values(\n",
    "    by=['GBE ID']\n",
    ").to_csv('20191110.duplicates.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We confirmed that all of the duplicated entries are consistent (i.e. it is fine to keep an arbitrary picked one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare existing ones with a proper merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_df = functools.reduce(\n",
    "    lambda x, y: merge_table(x, y, 'GBE ID'), \n",
    "    [my_read_table(x, cols) for x in existing_tables]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{nan: 402}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:v for k,v in collections.Counter(existing_df['GBE ID']).items() if v>1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concatenate with the new tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tables =[\n",
    "    'ukb_20191030.tsv', \n",
    "    'ukb_20191101.2004890.35059.tsv',\n",
    "    'ukb_20191101.2005223.37338.tsv'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = functools.reduce(\n",
    "    lambda x, y: merge_table(x, y, 'FieldID'), \n",
    "    [existing_df] + [my_read_table(x, cols) for x in new_tables]\n",
    ")[cols]\n",
    "merged_df['FieldID'] = merged_df['FieldID'].map(lambda x: int(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5788, 27)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.sort_values(by=['FieldID', 'GBE ID'])[cols].to_csv('ukb_20191110.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
